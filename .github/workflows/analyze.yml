
name: Generate Report

on:
  issues:
    types: [opened]

jobs:
  analyze:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1

      - name: Install dependencies
        run: |
          npm init -y
          npm install @lhci/cli@0.13.x cheerio linkinator node-fetch@3

      - name: Extract URL from issue (title/body)
        id: extract
        run: |
          TITLE=$(jq -r '.issue.title' "$GITHUB_EVENT_PATH")
          BODY=$(jq -r '.issue.body' "$GITHUB_EVENT_PATH")
          # جرّب العنوان أولاً
          URL=$(echo "$TITLE" | grep -oE 'https?://[^ ]+' || true)
          # ولو فشل، جرّب المتن
          if [ -z "$URL" ]; then
            URL=$(echo "$BODY" | grep -oE 'https?://[^ ]+' | head -n1 || true)
          fi
          if [ -z "$URL" ]; then
            echo "❌ لم يتم العثور على URL داخل الـ Issue."
            exit 1
          fi
          echo "URL=$URL" >> $GITHUB_ENV
          echo "✅ Using URL: $URL"

      - name: Create output folder
        run: mkdir -p reports/$GITHUB_RUN_ID

      - name: Run Lighthouse
        env:
          URL: ${{ env.URL }}
        run: |
          echo "Running Lighthouse for $URL"
          npx @lhci/cli collect \
            --url "$URL" \
            --numberOfRuns=1 \
            --outputDir="reports/$GITHUB_RUN_ID/lh" \
            --report=html,json \
            --settings.chromePath="$(which chrome)" \
            --settings.chromeFlags="--headless=new"

      - name: Scrape SEO elements
        run: |
          node << 'EOF'
          import fetch from 'node-fetch';
          import fs from 'fs';
          import cheerio from 'cheerio';

          const url = process.env.URL;
          const res = await fetch(url, {headers:{'User-Agent':'SEO-Auditor/1.0'}});
          const html = await res.text();
          const $ = cheerio.load(html);

          const data = {
            url,
            title: $('title').text() || '',
            metaDesc: $('meta[name="description"]').attr('content') || '',
            h1: $('h1').first().text() || ''
          };

          fs.writeFileSync(`reports/${process.env.GITHUB_RUN_ID}/seo.json`, JSON.stringify(data, null, 2));
          EOF

      - name: Check broken links
        run: |
          node << 'EOF'
          import { LinkChecker } from 'linkinator';
          import fs from 'fs';

          const checker = new LinkChecker();
          const res = await checker.check({
            path: process.env.URL,
            recurse: false,
            timeout: 15000
          });
          const broken = res.links.filter(l => l.state === 'BROKEN').map(l=>({url:l.url,status:l.status}));
          fs.writeFileSync(`reports/${process.env.GITHUB_RUN_ID}/links.json`, JSON.stringify({
            scanned: res.links.length,
            brokenCount: broken.length,
            broken
          }, null, 2));
          EOF

      - name: Upload report as artifact
        uses: actions/upload-artifact@v4
        with:
          name: report-${{ github.run_id }}
          path: reports/${{ github.run_id }}

      - name: Comment back with link
        env:
          RUN_ID: ${{ github.run_id }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh issue comment ${{ github.event.issue.number }} \
            --body "✅ Report ready! Download artifacts:\nhttps://github.com/${{ github.repository }}/actions/runs/${RUN_ID}"
